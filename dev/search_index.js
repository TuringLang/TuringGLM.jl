var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API reference","title":"API reference","text":"CurrentModule = TuringGLM","category":"page"},{"location":"api/#TuringGLM","page":"API reference","title":"TuringGLM","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Documentation for TuringGLM.","category":"page"},{"location":"api/","page":"API reference","title":"API reference","text":"","category":"page"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [TuringGLM]","category":"page"},{"location":"api/#TuringGLM.CustomPrior","page":"API reference","title":"TuringGLM.CustomPrior","text":"CustomPrior(predictors, intercept, auxiliary)\n\nstruct to hold information regarding user-specified custom priors.\n\nUsage\n\nThe CustomPrior struct has 3 fields:\n\npredictors: the β coefficients.\nintercept: the α intercept.\nauxiliary: an auxiliary parameter.\n\nIn robust models, e.g. Linear Regression with Student-t likelihood or Count Regression with Negative Binomial likelihood, often there is an extra auxiliary parameter that is needed to parametrize to model to overcome under- or over-dispersion. If you are specifying a custom prior for one of these type of models, then you should also specify a prior for the auxiliary parameter.\n\nNon-robust models do not need an auxiliary parameter and you can pass nothing as the auxiliary argument.\n\n\n\n\n\n","category":"type"},{"location":"api/#TuringGLM.NegativeBinomial2-Union{Tuple{T}, Tuple{T, T}} where T<:Real","page":"API reference","title":"TuringGLM.NegativeBinomial2","text":"NegativeBinomial2(μ, ϕ)\n\nAn alternative parameterization of the Negative Binomial distribution:\n\ntextNegative-Binomial(n mid mu phi) sim binomn + phi - 1n left( fracmumu + phi right)^n left( fracphimu + phi right)^phi\n\nwhere the expectation is μ and variance is (μ + μ²/ϕ).\n\nThe alternative parameterization is inspired by the Stan's neg_binomial_2 function.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.center_predictors-Tuple{AbstractMatrix{T} where T}","page":"API reference","title":"TuringGLM.center_predictors","text":"center_predictors(X::AbstractMatrix)\n\nCenters the columns of a matrix X of predictors to mean 0.\n\nReturns a tuple with:\n\nμ_X: 1xK Matrix of Float64s of the means of the K columns in the original X\n\nmatrix.\n\nX_centered: A Matrix of Float64s with the same dimensions as the original matrix\n\nX with the columns centered on mean μ=0.\n\nArguments\n\nX::AbstractMatrix: a matrix of predictors where rows are observations and columns are\n\nvariables.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.convert_str_to_indices-Tuple{AbstractVector{T} where T}","page":"API reference","title":"TuringGLM.convert_str_to_indices","text":"convert_str_to_indices(v::AbstractVector)\n\nConverts a vector v to a vector of indices, i.e. a vector where all the entries are integers. Returns a tuple with the first element as the converted vector and the second element a Dict specifying which string is which integer.\n\nThis function is especially useful for random-effects varying-intercept hierarchical models. Normally v would be a vector of group membership with values such as \"group_1\", \"group_2\" etc. For random-effect models with varying-intercepts, Turing needs the group membership values to be passed as Ints.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.data_fixed_effects-Union{Tuple{D}, Tuple{StatsModels.FormulaTerm, D}} where D","page":"API reference","title":"TuringGLM.data_fixed_effects","text":"data_fixed_effects(formula::FormulaTerm, data)\n\nConstructs the matrix X of fixed-effects (a.k.a. population-level) predictors.\n\nReturns a Matrix of the fixed-effects predictors variables in the formula and present inside data.\n\nArguments\n\nformula: a FormulaTerm created by @formula macro.\ndata:  a data object that satisfies the\n\nTables.jl interface such as a DataFrame.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.data_random_effects-Union{Tuple{D}, Tuple{StatsModels.FormulaTerm, D}} where D","page":"API reference","title":"TuringGLM.data_random_effects","text":"data_random_effects(formula::FormulaTerm, data)\n\nConstructs the vector(s)/matrix(ces) Z(s) of random-effects (a.k.a. group-level) slope predictors.\n\nReturns a Dict{String, AbstractArray} of Vector/Matrix as values of the random-effects predictors slope variables (keys) in the formula and present inside data.\n\nArguments\n\nformula: a FormulaTerm created by @formula macro.\ndata:  a data object that satisfies the\n\nTables.jl interface such as a DataFrame.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.data_response-Union{Tuple{D}, Tuple{StatsModels.FormulaTerm, D}} where D","page":"API reference","title":"TuringGLM.data_response","text":"data_response(formula::FormulaTerm, data)\n\nConstructs the response y vector.\n\nReturns a Vector of the response variable in the formula and present inside data.\n\nArguments\n\nformula: a FormulaTerm created by @formula macro.\ndata:  a data object that satisfies the\n\nTables.jl interface such as a DataFrame.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.get_idx-Union{Tuple{D}, Tuple{StatsModels.Term, D}} where D","page":"API reference","title":"TuringGLM.get_idx","text":"get_idx(term::Term, data)\n\nReturns a tuple with the first element as the ID vector of Ints that represent group membership for a specific random-effect intercept group t of observations present in data. The second element of the tuple is a Dict specifying which string is which integer in the ID vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.get_var-Union{Tuple{D}, Tuple{StatsModels.Term, D}} where D","page":"API reference","title":"TuringGLM.get_var","text":"get_var(term::Term, data)\n\nReturns the corresponding vector of column in data for the a specific random-effect slope term of observations present in data.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.has_ranef-Tuple{StatsModels.FormulaTerm}","page":"API reference","title":"TuringGLM.has_ranef","text":"has_ranef(formula::FormulaTerm)\n\nReturns true if any of the terms in formula is a FunctionTerm or false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.intercept_per_ranef-Tuple{Tuple}","page":"API reference","title":"TuringGLM.intercept_per_ranef","text":"intercept_per_ranef(terms::Tuple{RandomEffectsTerm})\n\nReturns a vector of Strings where the entries are the grouping variables that have a group-level intercept.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.n_ranef-Tuple{StatsModels.FormulaTerm}","page":"API reference","title":"TuringGLM.n_ranef","text":"n_ranef(formula::FormulaTerm)\n\nReturns the number of RandomEffectsTerms in formula.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.ranef-Tuple{StatsModels.FormulaTerm}","page":"API reference","title":"TuringGLM.ranef","text":"ranef(formula::FormulaTerm)\n\nReturns a tuple of the FunctionTerms parsed as RandomEffectsTerms in formula. If there are no FunctionTerms in formula returns nothing.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.slope_per_ranef-Tuple{Tuple}","page":"API reference","title":"TuringGLM.slope_per_ranef","text":"slope_per_ranef(terms::Tuple{RandomEffectsTerm})\n\nReturns a SlopePerRanEf object where the entries are the grouping variables that have a group-level slope.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.standardize_predictors-Tuple{AbstractMatrix{T} where T}","page":"API reference","title":"TuringGLM.standardize_predictors","text":"standardize_predictors(X::AbstractMatrix)\n\nStandardizes the columns of a matrix X of predictors to mean 0 and standard deviation 1.\n\nReturns a tuple with:\n\nμ_X: 1xK Matrix of Float64s of the means of the K columns in the original X\n\nmatrix.\n\nσ_X: 1xK Matrix of Float64s of the standard deviations of the K columns in the\n\noriginal X matrix.\n\nX_std: A Matrix of Float64s with the same dimensions as the original matrix\n\nX with the columns centered on mean μ=0 and standard deviation σ=1.\n\nArguments\n\nX::AbstractMatrix: a matrix of predictors where rows are observations and columns are\n\nvariables.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.standardize_predictors-Tuple{AbstractVector{T} where T}","page":"API reference","title":"TuringGLM.standardize_predictors","text":"standardize_predictors(x::AbstractVector)\n\nStandardizes the vector x to mean 0 and standard deviation 1.\n\nReturns a tuple with:\n\nμ_X: Float64s of the mean of the original vector x.\nσ_X: Float64s of the standard deviations of the original vector x.\nx_std: A Vector of Float64s with the same length as the original vector\n\nx with the values centered on mean μ=0 and standard deviation σ=1.\n\nArguments\n\nx::AbstractVector: a vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.tuple_length-Union{Tuple{Tuple{Vararg{Any, N}}}, Tuple{N}} where N","page":"API reference","title":"TuringGLM.tuple_length","text":"tuple_length(::NTuple{N, Any}) where {N} = Int(N)\n\nThis is a hack to get the length of any tuple.\n\n\n\n\n\n","category":"method"},{"location":"api/#TuringGLM.turing_model-Union{Tuple{D}, Tuple{StatsModels.FormulaTerm, D}} where D","page":"API reference","title":"TuringGLM.turing_model","text":"turing_model(formula, data; model=Gaussian(), priors=DefaultPrior(), standardize=false)\nturing_model(formula, data, model; priors=DefaultPrior(), standardize=false)\n\nCreate a Turing model using formula syntax and a data source.\n\nformula\n\nformula is the the same friendly interface to specify used to specify statistical models by brms, rstarnarm, bambi, StatsModels.jl and MixedModels.jl. The syntax is done by using the @formula macro and then specifying the dependent variable followed by a tilde ~ then the independent variables separated by a plus sign +.\n\nExample: @formula(y ~ x1 + x2 + x3).\n\nModerations/interactions can be specified with the asterisk sign *, e.g. x1 * x2. This will be expanded to x1 + x2 + x1:x2, which, following the principle of hierarchy, the main effects must also be added along with the interaction effects. Here x1:x2 means that the values of x1 will be multiplied (interacted) with the values of x2.\n\nRandom-effects (a.k.a. group-level effects) can be specified with the (term | group) inside the @formula, where term is the independent variable and group is the categorical representation (i.e., either a column of Strings or a CategoricalArray in data). You can specify a random-intercept with (1 | group).\n\nExample: @formula(y ~ (1 | group) + x1).\n\nNotice: random-effects are currently only implemented for a single group-level intercept. Future versions of TuringGLM.jl will support slope random-effects and multiple group-level effets.\n\ndata\n\ndata can be any Tables.jl-compatible data interface. The most popular ones are DataFrames and NamedTuples.\n\nmodel\n\nmodel represents the likelihood function which you want to condition your data on. Currently, TuringGLM.jl supports:\n\nGaussian() (the default if not specified): linear regression\nStudent(): robust linear regression\nLogistic(): logistic regression\nPois(): Poisson count data regression\nNegBin(): negative binomial robust count data regression\n\npriors\n\nTuringGLM.jl comes with state-of-the-art default priors, based on the literature and the Stan community. By default, turing_model will use DefaultPrior. But you can specify your own with priors=CustomPrior(predictors, intercept, auxiliary). All models take a predictors and intercept priors.\n\nIn robust models, e.g. Linear Regression with Student-t likelihood or Count Regression with Negative Binomial likelihood, often there is an extra auxiliary parameter that is needed to parametrize to model to overcome under- or over-dispersion. If you are specifying a custom prior for one of these type of models, then you should also specify a prior for the auxiliary parameter.\n\nNon-robust models do not need an auxiliary parameter and you can pass nothing as the auxiliary argument.\n\nExample for a non-robust model: @formula(y, ...), data; priors=CustomPrior(Normal(0, 2.5), Normal(10, 5), nothing)\n\nExample for a robust model: @formula(y, ...), data; priors=CustomPrior(Normal(0, 2.5), Normal(10, 5), Exponential(1))\n\nstandardize\n\nWhether true or false to standardize your data to mean 0 and standard deviation 1 before inference. Some science fields prefer to analyze and report effects in terms of standard devations. Also, whenever measurement scales differs, it is often suggested to standardize the effects for better comparison. By default, turing_model sets standardize=false.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/robust_regression/#Robust-Regression","page":"Robust Regression","title":"Robust Regression","text":"","category":"section"},{"location":"tutorials/robust_regression/","page":"Robust Regression","title":"Robust Regression","text":"# Auto generated file. Do not modify.","category":"page"},{"location":"tutorials/robust_regression/","page":"Robust Regression","title":"Robust Regression","text":"<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"82c7e3aa7460816c54cf75cde851537479a6c7836fac5cee7dd729faca8cf96d\"\n    julia_version = \"1.6.5\"\n-->\n\n\n\n\n<div class=\"markdown\"><p>For the <strong>Robust Regression</strong> with Student-<span class=\"tex\">$t$</span> distribution as the likelihood, we&#39;ll use a famous dataset called <code>kidiq</code> &#40;Gelman &amp; Hill, 2007&#41;, which is data from a survey of adult American women and their respective children. Dated from 2007, it has 434 observations and 4 variables:</p>\n<ul>\n<li><p><code>kid_score</code>: child&#39;s IQ</p>\n</li>\n<li><p><code>mom_hs</code>: binary/dummy &#40;0 or 1&#41; if the child&#39;s mother has a high school diploma</p>\n</li>\n<li><p><code>mom_iq</code>: mother&#39;s IQ</p>\n</li>\n<li><p><code>mom_age</code>: mother&#39;s age</p>\n</li>\n</ul>\n</div>\n\n<pre><code class=\"language-julia\">using CSV</code></pre>\n\n\n<pre><code class=\"language-julia\">using DataFrames</code></pre>\n\n\n<pre><code class=\"language-julia\">url = \"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/kidiq.csv\"</code></pre>\n<pre><code class=\"code-output\">\"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/kidiq.csv\"</code></pre>\n\n<pre><code class=\"language-julia\">kidiq = CSV.read(download(url), DataFrame)</code></pre>\n<table>\n<tr>\n<th>kid_score</th>\n<th>mom_hs</th>\n<th>mom_iq</th>\n<th>mom_age</th>\n</tr>\n<tr>\n<td>65</td>\n<td>1</td>\n<td>121.118</td>\n<td>27</td>\n</tr>\n<tr>\n<td>98</td>\n<td>1</td>\n<td>89.3619</td>\n<td>25</td>\n</tr>\n<tr>\n<td>85</td>\n<td>1</td>\n<td>115.443</td>\n<td>27</td>\n</tr>\n<tr>\n<td>83</td>\n<td>1</td>\n<td>99.4496</td>\n<td>25</td>\n</tr>\n<tr>\n<td>115</td>\n<td>1</td>\n<td>92.7457</td>\n<td>27</td>\n</tr>\n<tr>\n<td>98</td>\n<td>0</td>\n<td>107.902</td>\n<td>18</td>\n</tr>\n<tr>\n<td>69</td>\n<td>1</td>\n<td>138.893</td>\n<td>20</td>\n</tr>\n<tr>\n<td>106</td>\n<td>1</td>\n<td>125.145</td>\n<td>23</td>\n</tr>\n<tr>\n<td>102</td>\n<td>1</td>\n<td>81.6195</td>\n<td>24</td>\n</tr>\n<tr>\n<td>95</td>\n<td>1</td>\n<td>95.0731</td>\n<td>19</td>\n</tr>\n<tr>\n<td>...</td>\n</tr>\n<tr>\n<td>70</td>\n<td>1</td>\n<td>91.2533</td>\n<td>25</td>\n</tr>\n</table>\n\n\n<pre><code class=\"language-julia\">using TuringGLM</code></pre>\n\n\n\n<div class=\"markdown\"><p>Using <code>kid_score</code> as dependent variable and <code>mom_hs</code> along with <code>mom_iq</code> as independent variables with a moderation &#40;interaction&#41; effect:</p>\n</div>\n\n<pre><code class=\"language-julia\">fm = @formula(kid_score ~ mom_hs * mom_iq)</code></pre>\n<pre><code class=\"code-output\">FormulaTerm\nResponse:\n  kid_score(unknown)\nPredictors:\n  mom_hs(unknown)\n  mom_iq(unknown)\n  mom_hs(unknown) & mom_iq(unknown)</code></pre>\n\n\n<div class=\"markdown\"><p>We instantiate our model with <code>turing_model</code> passing a third argument <code>Student&#40;&#41;</code> to indicate that the model is a robust regression with the Student&#39;s t-distribution:</p>\n</div>\n\n<pre><code class=\"language-julia\">model = turing_model(fm, kidiq, Student());</code></pre>\n\n\n<pre><code class=\"language-julia\">chn = sample(model, NUTS(), 2_000);</code></pre>\n\n\n<pre><code class=\"language-julia\">describe(chn)[1]</code></pre>\n<table>\n<tr>\n<th>parameters</th>\n<th>mean</th>\n<th>std</th>\n<th>naive_se</th>\n<th>mcse</th>\n<th>ess</th>\n<th>rhat</th>\n<th>ess_per_sec</th>\n</tr>\n<tr>\n<td>:α</td>\n<td>40.7067</td>\n<td>6.6893</td>\n<td>0.149577</td>\n<td>0.388377</td>\n<td>247.819</td>\n<td>0.999635</td>\n<td>7.52494</td>\n</tr>\n<tr>\n<td>Symbol(\"β[1]\")</td>\n<td>2.74266</td>\n<td>6.37503</td>\n<td>0.14255</td>\n<td>0.438103</td>\n<td>174.804</td>\n<td>0.999809</td>\n<td>5.30785</td>\n</tr>\n<tr>\n<td>Symbol(\"β[2]\")</td>\n<td>0.468891</td>\n<td>0.0705477</td>\n<td>0.00157749</td>\n<td>0.00395597</td>\n<td>261.884</td>\n<td>0.999545</td>\n<td>7.95201</td>\n</tr>\n<tr>\n<td>Symbol(\"β[3]\")</td>\n<td>0.0132051</td>\n<td>0.0665945</td>\n<td>0.0014891</td>\n<td>0.00438244</td>\n<td>187.35</td>\n<td>0.99965</td>\n<td>5.68883</td>\n</tr>\n<tr>\n<td>:σ</td>\n<td>6.10887</td>\n<td>0.429816</td>\n<td>0.00961098</td>\n<td>0.013955</td>\n<td>838.465</td>\n<td>1.0047</td>\n<td>25.4597</td>\n</tr>\n<tr>\n<td>:ν</td>\n<td>1.04003</td>\n<td>0.0991404</td>\n<td>0.00221685</td>\n<td>0.00338653</td>\n<td>925.431</td>\n<td>1.00512</td>\n<td>28.1004</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><h2>References</h2>\n<p>Gelman, A., &amp; Hill, J. &#40;2007&#41;. Data analysis using regression and multilevel/hierarchical models. Cambridge university press.</p>\n</div>\n\n<!-- PlutoStaticHTML.End -->","category":"page"},{"location":"tutorials/negative_binomial_regression/#Negative-Binomial-Regression","page":"Negative Binomial Regression","title":"Negative Binomial Regression","text":"","category":"section"},{"location":"tutorials/negative_binomial_regression/","page":"Negative Binomial Regression","title":"Negative Binomial Regression","text":"# Auto generated file. Do not modify.","category":"page"},{"location":"tutorials/negative_binomial_regression/","page":"Negative Binomial Regression","title":"Negative Binomial Regression","text":"<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"0e6d6b517034002653a88b576cd3b1b8f1ff2fe1df35d5544843bb3d983777c2\"\n    julia_version = \"1.6.5\"\n-->\n\n\n\n\n<div class=\"markdown\"><p>For our example on <strong>Negative Binomial Regression</strong>, let&#39;s use a famous dataset called <code>roaches</code> &#40;Gelman &amp; Hill, 2007&#41;, which is data on the efficacy of a pest management system at reducing the number of roaches in urban apartments. It has 262 observations and the following variables:</p>\n<ul>\n<li><p><code>y</code> – number of roaches caught.</p>\n</li>\n<li><p><code>roach1</code> – pretreatment number of roaches.</p>\n</li>\n<li><p><code>treatment</code> – binary/dummy &#40;0 or 1&#41; for treatment indicator.</p>\n</li>\n<li><p><code>senior</code> – binary/dummy &#40;0 or 1&#41; for only elderly residents in building.</p>\n</li>\n<li><p><code>exposure2</code> – number of days for which the roach traps were used</p>\n</li>\n</ul>\n</div>\n\n<pre><code class=\"language-julia\">using CSV</code></pre>\n\n\n<pre><code class=\"language-julia\">using DataFrames</code></pre>\n\n\n<pre><code class=\"language-julia\">url = \"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/roaches.csv\";</code></pre>\n\n\n<pre><code class=\"language-julia\">roaches = CSV.read(download(url), DataFrame)</code></pre>\n<table>\n<tr>\n<th>y</th>\n<th>roach1</th>\n<th>treatment</th>\n<th>senior</th>\n<th>exposure2</th>\n</tr>\n<tr>\n<td>153</td>\n<td>308.0</td>\n<td>1</td>\n<td>0</td>\n<td>0.8</td>\n</tr>\n<tr>\n<td>127</td>\n<td>331.25</td>\n<td>1</td>\n<td>0</td>\n<td>0.6</td>\n</tr>\n<tr>\n<td>7</td>\n<td>1.67</td>\n<td>1</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>7</td>\n<td>3.0</td>\n<td>1</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>0</td>\n<td>2.0</td>\n<td>1</td>\n<td>0</td>\n<td>1.14286</td>\n</tr>\n<tr>\n<td>0</td>\n<td>0.0</td>\n<td>1</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>73</td>\n<td>70.0</td>\n<td>1</td>\n<td>0</td>\n<td>0.8</td>\n</tr>\n<tr>\n<td>24</td>\n<td>64.56</td>\n<td>1</td>\n<td>0</td>\n<td>1.14286</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1.0</td>\n<td>0</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>2</td>\n<td>14.0</td>\n<td>0</td>\n<td>0</td>\n<td>1.14286</td>\n</tr>\n<tr>\n<td>...</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.0</td>\n<td>0</td>\n<td>1</td>\n<td>1.0</td>\n</tr>\n</table>\n\n\n<pre><code class=\"language-julia\">using TuringGLM</code></pre>\n\n\n\n<div class=\"markdown\"><p>Using <code>y</code> as dependent variable and <code>roach1</code>, <code>treatment</code>, and <code>senior</code> as independent variables:</p>\n</div>\n\n<pre><code class=\"language-julia\">fm = @formula(y ~ roach1 + treatment + senior)</code></pre>\n<pre><code class=\"code-output\">FormulaTerm\nResponse:\n  y(unknown)\nPredictors:\n  roach1(unknown)\n  treatment(unknown)\n  senior(unknown)</code></pre>\n\n\n<div class=\"markdown\"><p>We instantiate our model with <code>turing_model</code> passing a third argument <code>NegBin&#40;&#41;</code> to indicate that the model is a negative binomial regression:</p>\n</div>\n\n<pre><code class=\"language-julia\">model = turing_model(fm, roaches, NegBin());</code></pre>\n\n\n<pre><code class=\"language-julia\">chn = sample(model, NUTS(), 2_000);</code></pre>\n\n\n<pre><code class=\"language-julia\">describe(chn)[1]</code></pre>\n<table>\n<tr>\n<th>parameters</th>\n<th>mean</th>\n<th>std</th>\n<th>naive_se</th>\n<th>mcse</th>\n<th>ess</th>\n<th>rhat</th>\n<th>ess_per_sec</th>\n</tr>\n<tr>\n<td>:α</td>\n<td>2.81989</td>\n<td>0.140718</td>\n<td>0.00314656</td>\n<td>0.00409425</td>\n<td>1099.86</td>\n<td>0.999591</td>\n<td>78.6175</td>\n</tr>\n<tr>\n<td>Symbol(\"β[1]\")</td>\n<td>0.0126516</td>\n<td>0.00154904</td>\n<td>3.46377e-5</td>\n<td>2.72087e-5</td>\n<td>2525.62</td>\n<td>0.999642</td>\n<td>180.53</td>\n</tr>\n<tr>\n<td>Symbol(\"β[2]\")</td>\n<td>-0.706434</td>\n<td>0.148864</td>\n<td>0.0033287</td>\n<td>0.00373138</td>\n<td>1278.15</td>\n<td>1.00027</td>\n<td>91.3615</td>\n</tr>\n<tr>\n<td>Symbol(\"β[3]\")</td>\n<td>-0.320216</td>\n<td>0.160948</td>\n<td>0.0035989</td>\n<td>0.00404964</td>\n<td>1411.87</td>\n<td>0.999622</td>\n<td>100.92</td>\n</tr>\n<tr>\n<td>:ϕ⁻</td>\n<td>1.40959</td>\n<td>0.0769243</td>\n<td>0.00172008</td>\n<td>0.00188587</td>\n<td>1538.51</td>\n<td>1.00038</td>\n<td>109.972</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><h2>References</h2>\n<p>Gelman, A., &amp; Hill, J. &#40;2007&#41;. Data analysis using regression and multilevel/hierarchical models. Cambridge university press.</p>\n</div>\n\n<!-- PlutoStaticHTML.End -->","category":"page"},{"location":"tutorials/custom_priors/#Custom-Priors","page":"Custom Priors","title":"Custom Priors","text":"","category":"section"},{"location":"tutorials/custom_priors/","page":"Custom Priors","title":"Custom Priors","text":"# Auto generated file. Do not modify.","category":"page"},{"location":"tutorials/custom_priors/","page":"Custom Priors","title":"Custom Priors","text":"<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"ec057d0889b7f52703e45c4e6ca14af7e0ad758bdb3a5d33dfeba709366fc053\"\n    julia_version = \"1.6.5\"\n-->\n\n\n\n\n<div class=\"markdown\"><p>Let&#39;s cover the <strong>Linear Regression</strong> example with the <code>kidiq</code> dataset &#40;Gelman &amp; Hill, 2007&#41;, which is data from a survey of adult American women and their respective children. Dated from 2007, it has 434 observations and 4 variables:</p>\n<ul>\n<li><p><code>kid_score</code>: child&#39;s IQ</p>\n</li>\n<li><p><code>mom_hs</code>: binary/dummy &#40;0 or 1&#41; if the child&#39;s mother has a high school diploma</p>\n</li>\n<li><p><code>mom_iq</code>: mother&#39;s IQ</p>\n</li>\n<li><p><code>mom_age</code>: mother&#39;s age</p>\n</li>\n</ul>\n</div>\n\n<pre><code class=\"language-julia\">using CSV</code></pre>\n\n\n<pre><code class=\"language-julia\">using DataFrames</code></pre>\n\n\n<pre><code class=\"language-julia\">url = \"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/kidiq.csv\"</code></pre>\n<pre><code class=\"code-output\">\"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/kidiq.csv\"</code></pre>\n\n<pre><code class=\"language-julia\">kidiq = CSV.read(download(url), DataFrame)</code></pre>\n<table>\n<tr>\n<th>kid_score</th>\n<th>mom_hs</th>\n<th>mom_iq</th>\n<th>mom_age</th>\n</tr>\n<tr>\n<td>65</td>\n<td>1</td>\n<td>121.118</td>\n<td>27</td>\n</tr>\n<tr>\n<td>98</td>\n<td>1</td>\n<td>89.3619</td>\n<td>25</td>\n</tr>\n<tr>\n<td>85</td>\n<td>1</td>\n<td>115.443</td>\n<td>27</td>\n</tr>\n<tr>\n<td>83</td>\n<td>1</td>\n<td>99.4496</td>\n<td>25</td>\n</tr>\n<tr>\n<td>115</td>\n<td>1</td>\n<td>92.7457</td>\n<td>27</td>\n</tr>\n<tr>\n<td>98</td>\n<td>0</td>\n<td>107.902</td>\n<td>18</td>\n</tr>\n<tr>\n<td>69</td>\n<td>1</td>\n<td>138.893</td>\n<td>20</td>\n</tr>\n<tr>\n<td>106</td>\n<td>1</td>\n<td>125.145</td>\n<td>23</td>\n</tr>\n<tr>\n<td>102</td>\n<td>1</td>\n<td>81.6195</td>\n<td>24</td>\n</tr>\n<tr>\n<td>95</td>\n<td>1</td>\n<td>95.0731</td>\n<td>19</td>\n</tr>\n<tr>\n<td>...</td>\n</tr>\n<tr>\n<td>70</td>\n<td>1</td>\n<td>91.2533</td>\n<td>25</td>\n</tr>\n</table>\n\n\n<pre><code class=\"language-julia\">using TuringGLM</code></pre>\n\n\n\n<div class=\"markdown\"><p>Using <code>kid_score</code> as dependent variable and <code>mom_hs</code> along with <code>mom_iq</code> as independent variables with a moderation &#40;interaction&#41; effect:</p>\n</div>\n\n<pre><code class=\"language-julia\">fm = @formula(kid_score ~ mom_hs * mom_iq)</code></pre>\n<pre><code class=\"code-output\">FormulaTerm\nResponse:\n  kid_score(unknown)\nPredictors:\n  mom_hs(unknown)\n  mom_iq(unknown)\n  mom_hs(unknown) & mom_iq(unknown)</code></pre>\n\n\n<div class=\"markdown\"><p>Let&#39;s create our CustomPrior object. No need for the third &#40;auxiliary&#41; prior for this model so we leave it as <code>nothing</code>:</p>\n</div>\n\n<pre><code class=\"language-julia\">priors = CustomPrior(Normal(0, 2.5), Normal(10, 20), nothing);</code></pre>\n\n\n\n<div class=\"markdown\"><p>We instantiate our model with <code>turing_model</code> without specifying any model, thus the default model will be used: <code>Gaussian&#40;&#41;</code>. Notice that we are specifying the <code>priors</code> keyword argument:</p>\n</div>\n\n<pre><code class=\"language-julia\">model = turing_model(fm, kidiq; priors);</code></pre>\n\n\n<pre><code class=\"language-julia\">chn = sample(model, NUTS(), 2_000);</code></pre>\n\n\n<pre><code class=\"language-julia\">describe(chn)[1]</code></pre>\n<table>\n<tr>\n<th>parameters</th>\n<th>mean</th>\n<th>std</th>\n<th>naive_se</th>\n<th>mcse</th>\n<th>ess</th>\n<th>rhat</th>\n<th>ess_per_sec</th>\n</tr>\n<tr>\n<td>:α</td>\n<td>27.1443</td>\n<td>4.93427</td>\n<td>0.110334</td>\n<td>0.193495</td>\n<td>662.891</td>\n<td>0.999501</td>\n<td>22.6738</td>\n</tr>\n<tr>\n<td>Symbol(\"β[1]\")</td>\n<td>2.26667</td>\n<td>2.43125</td>\n<td>0.0543643</td>\n<td>0.0746235</td>\n<td>1035.33</td>\n<td>0.9995</td>\n<td>35.4129</td>\n</tr>\n<tr>\n<td>Symbol(\"β[2]\")</td>\n<td>0.556306</td>\n<td>0.0547627</td>\n<td>0.00122453</td>\n<td>0.00223918</td>\n<td>626.354</td>\n<td>0.999503</td>\n<td>21.4241</td>\n</tr>\n<tr>\n<td>Symbol(\"β[3]\")</td>\n<td>0.0279708</td>\n<td>0.0311345</td>\n<td>0.000696189</td>\n<td>0.00107123</td>\n<td>879.432</td>\n<td>0.99951</td>\n<td>30.0804</td>\n</tr>\n<tr>\n<td>:σ</td>\n<td>14.0758</td>\n<td>0.335659</td>\n<td>0.00750556</td>\n<td>0.0081254</td>\n<td>1336.99</td>\n<td>0.999722</td>\n<td>45.731</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><h2>References</h2>\n<p>Gelman, A., &amp; Hill, J. &#40;2007&#41;. Data analysis using regression and multilevel/hierarchical models. Cambridge university press.</p>\n</div>\n\n<!-- PlutoStaticHTML.End -->","category":"page"},{"location":"tutorials/hierarchical_models/#Hierarchical-Models","page":"Hierarchical Models","title":"Hierarchical Models","text":"","category":"section"},{"location":"tutorials/hierarchical_models/","page":"Hierarchical Models","title":"Hierarchical Models","text":"# Auto generated file. Do not modify.","category":"page"},{"location":"tutorials/hierarchical_models/","page":"Hierarchical Models","title":"Hierarchical Models","text":"<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"63f61b21aeec84418409671b11c988f589badd28bf266587a694783593546cad\"\n    julia_version = \"1.6.5\"\n-->\n\n\n\n\n<div class=\"markdown\"><p>Currently, TuringGLM only supports hierarchical models with a single random-intercept. This is done by using the <code>&#40;1 | group&#41;</code> inside the <code>@formula</code> macro.</p>\n<p>For our <strong>Hierarchical Model</strong> example, let&#39;s use a famous dataset called <code>cheese</code> &#40;Boatwright, McCulloch &amp; Rossi, 1999&#41;, which is data from cheese ratings. A group of 10 rural and 10 urban raters rated 4 types of different cheeses &#40;A, B, C and D&#41; in two samples. So we have <span class=\"tex\">$4 \\cdot 20 \\cdot 2 &#61; 160$</span> observations and 4 variables:</p>\n<ul>\n<li><p><code>cheese</code>: type of cheese from <code>A</code> to <code>D</code></p>\n</li>\n<li><p><code>rater</code>: id of the rater from <code>1</code> to <code>10</code></p>\n</li>\n<li><p><code>background</code>: type of rater, either <code>rural</code> or <code>urban</code></p>\n</li>\n<li><p><code>y</code>: rating of the cheese</p>\n</li>\n</ul>\n</div>\n\n<pre><code class=\"language-julia\">using CSV</code></pre>\n\n\n<pre><code class=\"language-julia\">using DataFrames</code></pre>\n\n\n<pre><code class=\"language-julia\">url = \"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/cheese.csv\";</code></pre>\n\n\n<pre><code class=\"language-julia\">cheese = CSV.read(download(url), DataFrame)</code></pre>\n<table>\n<tr>\n<th>cheese</th>\n<th>rater</th>\n<th>background</th>\n<th>y</th>\n</tr>\n<tr>\n<td>\"A\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>67</td>\n</tr>\n<tr>\n<td>\"A\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>66</td>\n</tr>\n<tr>\n<td>\"B\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>51</td>\n</tr>\n<tr>\n<td>\"B\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>53</td>\n</tr>\n<tr>\n<td>\"C\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>75</td>\n</tr>\n<tr>\n<td>\"C\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>70</td>\n</tr>\n<tr>\n<td>\"D\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>68</td>\n</tr>\n<tr>\n<td>\"D\"</td>\n<td>1</td>\n<td>\"rural\"</td>\n<td>66</td>\n</tr>\n<tr>\n<td>\"A\"</td>\n<td>2</td>\n<td>\"rural\"</td>\n<td>76</td>\n</tr>\n<tr>\n<td>\"A\"</td>\n<td>2</td>\n<td>\"rural\"</td>\n<td>76</td>\n</tr>\n<tr>\n<td>...</td>\n</tr>\n<tr>\n<td>\"D\"</td>\n<td>10</td>\n<td>\"urban\"</td>\n<td>83</td>\n</tr>\n</table>\n\n\n<pre><code class=\"language-julia\">using TuringGLM</code></pre>\n\n\n\n<div class=\"markdown\"><p>Using <code>y</code> as dependent variable and <code>background</code> is independent variable with a varying-intercept per <code>cheese</code> type:</p>\n</div>\n\n<pre><code class=\"language-julia\">fm = @formula(y ~ (1 | cheese) + background)</code></pre>\n<pre><code class=\"code-output\">FormulaTerm\nResponse:\n  y(unknown)\nPredictors:\n  (cheese)->1 | cheese\n  background(unknown)</code></pre>\n\n\n<div class=\"markdown\"><p>We instantiate our model with <code>turing_model</code> without specifying any model, thus the default model will be used: <code>Gaussian&#40;&#41;</code></p>\n</div>\n\n<pre><code class=\"language-julia\">model = turing_model(fm, cheese);</code></pre>\n\n\n<pre><code class=\"language-julia\">chn = sample(model, NUTS(), 2_000);</code></pre>\n\n\n<pre><code class=\"language-julia\">describe(chn)[1]</code></pre>\n<table>\n<tr>\n<th>parameters</th>\n<th>mean</th>\n<th>std</th>\n<th>naive_se</th>\n<th>mcse</th>\n<th>ess</th>\n<th>rhat</th>\n<th>ess_per_sec</th>\n</tr>\n<tr>\n<td>:α</td>\n<td>68.0555</td>\n<td>5.02731</td>\n<td>0.112414</td>\n<td>0.224129</td>\n<td>462.739</td>\n<td>1.00106</td>\n<td>19.2832</td>\n</tr>\n<tr>\n<td>Symbol(\"β[1]\")</td>\n<td>6.90792</td>\n<td>0.930882</td>\n<td>0.0208152</td>\n<td>0.0254336</td>\n<td>1361.33</td>\n<td>0.999594</td>\n<td>56.7291</td>\n</tr>\n<tr>\n<td>:σ</td>\n<td>5.88541</td>\n<td>0.257308</td>\n<td>0.00575358</td>\n<td>0.00805492</td>\n<td>906.528</td>\n<td>0.999593</td>\n<td>37.7767</td>\n</tr>\n<tr>\n<td>:τ</td>\n<td>11.7362</td>\n<td>4.33598</td>\n<td>0.0969555</td>\n<td>0.225553</td>\n<td>360.481</td>\n<td>1.00753</td>\n<td>15.0219</td>\n</tr>\n<tr>\n<td>Symbol(\"zⱼ[1]\")</td>\n<td>0.31407</td>\n<td>0.451253</td>\n<td>0.0100903</td>\n<td>0.0200106</td>\n<td>509.303</td>\n<td>1.00012</td>\n<td>21.2236</td>\n</tr>\n<tr>\n<td>Symbol(\"zⱼ[2]\")</td>\n<td>-1.40864</td>\n<td>0.631433</td>\n<td>0.0141193</td>\n<td>0.0292995</td>\n<td>495.267</td>\n<td>1.0039</td>\n<td>20.6387</td>\n</tr>\n<tr>\n<td>Symbol(\"zⱼ[3]\")</td>\n<td>0.768017</td>\n<td>0.505874</td>\n<td>0.0113117</td>\n<td>0.022547</td>\n<td>520.918</td>\n<td>0.999761</td>\n<td>21.7076</td>\n</tr>\n<tr>\n<td>Symbol(\"zⱼ[4]\")</td>\n<td>0.0772599</td>\n<td>0.437364</td>\n<td>0.00977976</td>\n<td>0.0193559</td>\n<td>502.0</td>\n<td>1.00148</td>\n<td>20.9193</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><h2>References</h2>\n<p>Boatwright, P., McCulloch, R., &amp; Rossi, P. &#40;1999&#41;. Account-level modeling for trade promotion: An application of a constrained parameter hierarchical model. Journal of the American Statistical Association, 94&#40;448&#41;, 1063–1073.</p>\n</div>\n\n<!-- PlutoStaticHTML.End -->","category":"page"},{"location":"tutorials/linear_regression/#Linear-Regression","page":"Linear Regression","title":"Linear Regression","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"# Auto generated file. Do not modify.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"2c83c92e0223fcf069f88aceb6771b5e46779d3b8609fab91df569f44c351f50\"\n    julia_version = \"1.6.5\"\n-->\n\n\n\n\n<div class=\"markdown\"><p>Let&#39;s cover <strong>Linear Regression</strong> with a famous dataset called <code>kidiq</code> &#40;Gelman &amp; Hill, 2007&#41;, which is data from a survey of adult American women and their respective children. Dated from 2007, it has 434 observations and 4 variables:</p>\n<ul>\n<li><p><code>kid_score</code>: child&#39;s IQ</p>\n</li>\n<li><p><code>mom_hs</code>: binary/dummy &#40;0 or 1&#41; if the child&#39;s mother has a high school diploma</p>\n</li>\n<li><p><code>mom_iq</code>: mother&#39;s IQ</p>\n</li>\n<li><p><code>mom_age</code>: mother&#39;s age</p>\n</li>\n</ul>\n<p>For the purposes of this tutorial, we download the dataset from the TuringGLM repository:</p>\n</div>\n\n<pre><code class=\"language-julia\">using CSV</code></pre>\n\n\n<pre><code class=\"language-julia\">using DataFrames</code></pre>\n\n\n<pre><code class=\"language-julia\">using TuringGLM</code></pre>\n\n\n<pre><code class=\"language-julia\">url = \"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/kidiq.csv\";</code></pre>\n\n\n<pre><code class=\"language-julia\">kidiq = CSV.read(download(url), DataFrame)</code></pre>\n<table>\n<tr>\n<th>kid_score</th>\n<th>mom_hs</th>\n<th>mom_iq</th>\n<th>mom_age</th>\n</tr>\n<tr>\n<td>65</td>\n<td>1</td>\n<td>121.118</td>\n<td>27</td>\n</tr>\n<tr>\n<td>98</td>\n<td>1</td>\n<td>89.3619</td>\n<td>25</td>\n</tr>\n<tr>\n<td>85</td>\n<td>1</td>\n<td>115.443</td>\n<td>27</td>\n</tr>\n<tr>\n<td>83</td>\n<td>1</td>\n<td>99.4496</td>\n<td>25</td>\n</tr>\n<tr>\n<td>115</td>\n<td>1</td>\n<td>92.7457</td>\n<td>27</td>\n</tr>\n<tr>\n<td>98</td>\n<td>0</td>\n<td>107.902</td>\n<td>18</td>\n</tr>\n<tr>\n<td>69</td>\n<td>1</td>\n<td>138.893</td>\n<td>20</td>\n</tr>\n<tr>\n<td>106</td>\n<td>1</td>\n<td>125.145</td>\n<td>23</td>\n</tr>\n<tr>\n<td>102</td>\n<td>1</td>\n<td>81.6195</td>\n<td>24</td>\n</tr>\n<tr>\n<td>95</td>\n<td>1</td>\n<td>95.0731</td>\n<td>19</td>\n</tr>\n<tr>\n<td>...</td>\n</tr>\n<tr>\n<td>70</td>\n<td>1</td>\n<td>91.2533</td>\n<td>25</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><p>Using <code>kid_score</code> as dependent variable and <code>mom_hs</code> along with <code>mom_iq</code> as independent variables with a moderation &#40;interaction&#41; effect:</p>\n</div>\n\n<pre><code class=\"language-julia\">fm = @formula(kid_score ~ mom_hs * mom_iq)</code></pre>\n<pre><code class=\"code-output\">FormulaTerm\nResponse:\n  kid_score(unknown)\nPredictors:\n  mom_hs(unknown)\n  mom_iq(unknown)\n  mom_hs(unknown) & mom_iq(unknown)</code></pre>\n\n\n<div class=\"markdown\"><p>Next, we instantiate our model with <code>turing_model</code> without specifying any model, thus the default model will be used: <code>Gaussian&#40;&#41;</code>:</p>\n</div>\n\n<pre><code class=\"language-julia\">model = turing_model(fm, kidiq);</code></pre>\n\n\n<pre><code class=\"language-julia\">n_samples = 2_000;</code></pre>\n\n\n\n<div class=\"markdown\"><p>This model is a valid Turing model, which we can pass to the default <code>sample</code> function from Turing to get our parameter estimates. We use the <code>NUTS</code> sampler with 2000 samples.</p>\n</div>\n\n<pre><code class=\"language-julia\">chn = sample(model, NUTS(), n_samples);</code></pre>\n\n\n<pre><code class=\"language-julia\">describe(chn)[1]</code></pre>\n<table>\n<tr>\n<th>parameters</th>\n<th>mean</th>\n<th>std</th>\n<th>naive_se</th>\n<th>mcse</th>\n<th>ess</th>\n<th>rhat</th>\n<th>ess_per_sec</th>\n</tr>\n<tr>\n<td>:α</td>\n<td>28.0971</td>\n<td>8.76682</td>\n<td>0.196032</td>\n<td>0.798802</td>\n<td>63.7411</td>\n<td>1.03034</td>\n<td>4.37782</td>\n</tr>\n<tr>\n<td>Symbol(\"β[1]\")</td>\n<td>3.63947</td>\n<td>8.94124</td>\n<td>0.199932</td>\n<td>0.907645</td>\n<td>50.4714</td>\n<td>1.04016</td>\n<td>3.46644</td>\n</tr>\n<tr>\n<td>Symbol(\"β[2]\")</td>\n<td>0.546486</td>\n<td>0.0949245</td>\n<td>0.00212258</td>\n<td>0.00860934</td>\n<td>65.1915</td>\n<td>1.02965</td>\n<td>4.47744</td>\n</tr>\n<tr>\n<td>Symbol(\"β[3]\")</td>\n<td>0.0156625</td>\n<td>0.0956093</td>\n<td>0.00213789</td>\n<td>0.00964457</td>\n<td>51.4333</td>\n<td>1.0386</td>\n<td>3.53251</td>\n</tr>\n<tr>\n<td>:σ</td>\n<td>14.0746</td>\n<td>0.323586</td>\n<td>0.00723561</td>\n<td>0.00806088</td>\n<td>1377.35</td>\n<td>1.00036</td>\n<td>94.598</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><h2>References</h2>\n<p>Gelman, A., &amp; Hill, J. &#40;2007&#41;. Data analysis using regression and multilevel/hierarchical models. Cambridge university press.</p>\n</div>\n\n<!-- PlutoStaticHTML.End -->","category":"page"},{"location":"#TuringGLM","page":"Home","title":"TuringGLM","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for TuringGLM. Please file an issue  if you run into any problems.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TuringGLM makes easy to specify Bayesian Generalized Linear Models using the formula syntax and returns an instantiated Turing model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Heavily inspired by brms (uses RStan or CmdStanR) and bambi (uses PyMC3).","category":"page"},{"location":"#@formula","page":"Home","title":"@formula","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The @formula macro is extended from StatsModels.jl along with  MixedModels.jl for the random-effects (a.k.a. group-level predictors).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The syntax is done by using the @formula macro and then specifying the dependent variable followed by a tilde ~ then the independent variables separated by a plus sign +.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@formula(y ~ x1 + x2 + x3)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Moderations/interactions can be specified with the asterisk sign *, e.g. x1 * x2. This will be expanded to x1 + x2 + x1:x2, which, following the principle of hierarchy, the main effects must also be added along with the interaction effects. Here x1:x2 means that the values of x1 will be multiplied (interacted) with the values of x2.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Random-effects (a.k.a. group-level effects) can be specified with the (term | group) inside the @formula, where term is the independent variable and group is the categorical representation (i.e., either a column of Strings or a CategoricalArray in data). You can specify a random-intercept with (1 | group).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@formula(y ~ (1 | group) + x1)","category":"page"},{"location":"#Data","page":"Home","title":"Data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TuringGLM supports any Tables.jl-compatible data interface. The most popular ones are DataFrames and NamedTuples.","category":"page"},{"location":"#Supported-Models","page":"Home","title":"Supported Models","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TuringGLM supports non-hiearchical and hierarchical models. For hierarchical models, only single random-intercept hierarchical models are supported.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For likelihoods, TuringGLM.jl supports:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Gaussian() (the default if not specified): linear regression\nStudent(): robust linear regression\nLogistic(): logistic regression\nPois(): Poisson count data regression\nNegBin(): negative binomial robust count data regression","category":"page"},{"location":"#Tutorials","page":"Home","title":"Tutorials","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Take a look at the tutorials for all supported likelihood and models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"tutorials/linear_regression.md\",\n    \"tutorials/logistic_regression.md\",\n    \"tutorials/poisson_regression.md\",\n    \"tutorials/negativebinomial_regression.md\",\n    \"tutorials/robust_regression.md\",\n    \"tutorials/hierarchical_models.md\",\n    \"tutorials/custom_priors.md\"\n]\nDepth = 1","category":"page"},{"location":"tutorials/logistic_regression/#Logistic-Regression","page":"Logistic Regression","title":"Logistic Regression","text":"","category":"section"},{"location":"tutorials/logistic_regression/","page":"Logistic Regression","title":"Logistic Regression","text":"# Auto generated file. Do not modify.","category":"page"},{"location":"tutorials/logistic_regression/","page":"Logistic Regression","title":"Logistic Regression","text":"<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"508eb3c5bb65e7fe0a1e2c675803a38d8f8ad41266a99ea384344d81632f4ec4\"\n    julia_version = \"1.6.5\"\n-->\n\n\n\n\n<div class=\"markdown\"><p>For our tutorial on <strong>Logistic Regression</strong>, let&#39;s use a famous dataset called <code>wells</code> &#40;Gelman &amp; Hill, 2007&#41;, which is data from a survey of 3,200 residents in a small area of Bangladesh suffering from arsenic contamination of groundwater. Respondents with elevated arsenic levels in their wells had been encouraged to switch their water source to a safe public or private well in the nearby area and the survey was conducted several years later to learn which of the affected residents had switched wells. It has 3,200 observations and the following variables:</p>\n<ul>\n<li><p><code>switch</code> – binary/dummy &#40;0 or 1&#41; for well-switching.</p>\n</li>\n<li><p><code>arsenic</code> – arsenic level in respondent&#39;s well.</p>\n</li>\n<li><p><code>dist</code> – distance &#40;meters&#41; from the respondent&#39;s house to the nearest well with safe drinking water.</p>\n</li>\n<li><p><code>association</code> – binary/dummy &#40;0 or 1&#41; if member&#40;s&#41; of household participate in community organizations.</p>\n</li>\n<li><p><code>educ</code> – years of education &#40;head of household&#41;.</p>\n</li>\n</ul>\n</div>\n\n<pre><code class=\"language-julia\">using CSV</code></pre>\n\n\n<pre><code class=\"language-julia\">using DataFrames</code></pre>\n\n\n<pre><code class=\"language-julia\">using TuringGLM</code></pre>\n\n\n<pre><code class=\"language-julia\">url = \"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/wells.csv\";</code></pre>\n\n\n<pre><code class=\"language-julia\">wells = CSV.read(download(url), DataFrame)</code></pre>\n<table>\n<tr>\n<th>switch</th>\n<th>arsenic</th>\n<th>dist</th>\n<th>assoc</th>\n<th>educ</th>\n</tr>\n<tr>\n<td>1</td>\n<td>2.36</td>\n<td>16.826</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0.71</td>\n<td>47.322</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>0</td>\n<td>2.07</td>\n<td>20.967</td>\n<td>0</td>\n<td>10</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1.15</td>\n<td>21.486</td>\n<td>0</td>\n<td>12</td>\n</tr>\n<tr>\n<td>1</td>\n<td>1.1</td>\n<td>40.874</td>\n<td>1</td>\n<td>14</td>\n</tr>\n<tr>\n<td>1</td>\n<td>3.9</td>\n<td>69.518</td>\n<td>1</td>\n<td>9</td>\n</tr>\n<tr>\n<td>1</td>\n<td>2.97</td>\n<td>80.711</td>\n<td>1</td>\n<td>4</td>\n</tr>\n<tr>\n<td>1</td>\n<td>3.24</td>\n<td>55.146</td>\n<td>0</td>\n<td>10</td>\n</tr>\n<tr>\n<td>1</td>\n<td>3.28</td>\n<td>52.647</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>2.52</td>\n<td>75.072</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>...</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0.66</td>\n<td>20.844</td>\n<td>1</td>\n<td>5</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><p>Using <code>switch</code> as dependent variable and <code>dist</code>, <code>arsenic</code>, <code>assoc</code>, and <code>educ</code> as independent variables:</p>\n</div>\n\n<pre><code class=\"language-julia\">fm = @formula(switch ~ dist + arsenic + assoc + educ)</code></pre>\n<pre><code class=\"code-output\">FormulaTerm\nResponse:\n  switch(unknown)\nPredictors:\n  dist(unknown)\n  arsenic(unknown)\n  assoc(unknown)\n  educ(unknown)</code></pre>\n\n\n<div class=\"markdown\"><p>Now we instantiate our model with <code>turing_model</code> passing a third argument <code>Logistic&#40;&#41;</code> to indicate that the model is a logistic regression:</p>\n</div>\n\n<pre><code class=\"language-julia\">model = turing_model(fm, wells, Logistic());</code></pre>\n\n\n<pre><code class=\"language-julia\">chn = sample(model, NUTS(), 2_000);</code></pre>\n\n\n<pre><code class=\"language-julia\">describe(chn)[1]</code></pre>\n<table>\n<tr>\n<th>parameters</th>\n<th>mean</th>\n<th>std</th>\n<th>naive_se</th>\n<th>mcse</th>\n<th>ess</th>\n<th>rhat</th>\n<th>ess_per_sec</th>\n</tr>\n<tr>\n<td>:α</td>\n<td>-0.154951</td>\n<td>0.103572</td>\n<td>0.00231594</td>\n<td>0.00240488</td>\n<td>1185.49</td>\n<td>0.9995</td>\n<td>33.2908</td>\n</tr>\n<tr>\n<td>Symbol(\"β[1]\")</td>\n<td>-0.00900223</td>\n<td>0.00107444</td>\n<td>2.40253e-5</td>\n<td>2.07533e-5</td>\n<td>2604.52</td>\n<td>1.00185</td>\n<td>73.1403</td>\n</tr>\n<tr>\n<td>Symbol(\"β[2]\")</td>\n<td>0.468172</td>\n<td>0.0423384</td>\n<td>0.000946715</td>\n<td>0.0012681</td>\n<td>1103.2</td>\n<td>0.999658</td>\n<td>30.9799</td>\n</tr>\n<tr>\n<td>Symbol(\"β[3]\")</td>\n<td>-0.123194</td>\n<td>0.0775612</td>\n<td>0.00173432</td>\n<td>0.0017076</td>\n<td>1776.5</td>\n<td>1.00063</td>\n<td>49.8878</td>\n</tr>\n<tr>\n<td>Symbol(\"β[4]\")</td>\n<td>0.0425124</td>\n<td>0.00969373</td>\n<td>0.000216758</td>\n<td>0.000206258</td>\n<td>1571.3</td>\n<td>0.9995</td>\n<td>44.1253</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><h2>References</h2>\n<p>Gelman, A., &amp; Hill, J. &#40;2007&#41;. Data analysis using regression and multilevel/hierarchical models. Cambridge university press.</p>\n</div>\n\n<!-- PlutoStaticHTML.End -->","category":"page"},{"location":"tutorials/poisson_regression/#Poisson-Regression","page":"Poisson Regression","title":"Poisson Regression","text":"","category":"section"},{"location":"tutorials/poisson_regression/","page":"Poisson Regression","title":"Poisson Regression","text":"# Auto generated file. Do not modify.","category":"page"},{"location":"tutorials/poisson_regression/","page":"Poisson Regression","title":"Poisson Regression","text":"<!-- PlutoStaticHTML.Begin -->\n<!--\n    # This information is used for caching.\n    [PlutoStaticHTML.State]\n    input_sha = \"21486e92c12fe953052c606d656ae518497a22b0afcc978193c47949a50186a1\"\n    julia_version = \"1.6.5\"\n-->\n\n\n\n\n<div class=\"markdown\"><p>For our example on <strong>Poisson Regression</strong>, let&#39;s use a famous dataset called <code>roaches</code> &#40;Gelman &amp; Hill, 2007&#41;, which is data on the efficacy of a pest management system at reducing the number of roaches in urban apartments. It has 262 observations and the following variables:</p>\n<ul>\n<li><p><code>y</code> – number of roaches caught.</p>\n</li>\n<li><p><code>roach1</code> – pretreatment number of roaches.</p>\n</li>\n<li><p><code>treatment</code> – binary/dummy &#40;0 or 1&#41; for treatment indicator.</p>\n</li>\n<li><p><code>senior</code> – binary/dummy &#40;0 or 1&#41; for only elderly residents in building.</p>\n</li>\n<li><p><code>exposure2</code> – number of days for which the roach traps were used</p>\n</li>\n</ul>\n</div>\n\n<pre><code class=\"language-julia\">using CSV</code></pre>\n\n\n<pre><code class=\"language-julia\">using DataFrames</code></pre>\n\n\n<pre><code class=\"language-julia\">url = \"https://github.com/TuringLang/TuringGLM.jl/raw/main/data/roaches.csv\";</code></pre>\n\n\n<pre><code class=\"language-julia\">roaches = CSV.read(download(url), DataFrame)</code></pre>\n<table>\n<tr>\n<th>y</th>\n<th>roach1</th>\n<th>treatment</th>\n<th>senior</th>\n<th>exposure2</th>\n</tr>\n<tr>\n<td>153</td>\n<td>308.0</td>\n<td>1</td>\n<td>0</td>\n<td>0.8</td>\n</tr>\n<tr>\n<td>127</td>\n<td>331.25</td>\n<td>1</td>\n<td>0</td>\n<td>0.6</td>\n</tr>\n<tr>\n<td>7</td>\n<td>1.67</td>\n<td>1</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>7</td>\n<td>3.0</td>\n<td>1</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>0</td>\n<td>2.0</td>\n<td>1</td>\n<td>0</td>\n<td>1.14286</td>\n</tr>\n<tr>\n<td>0</td>\n<td>0.0</td>\n<td>1</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>73</td>\n<td>70.0</td>\n<td>1</td>\n<td>0</td>\n<td>0.8</td>\n</tr>\n<tr>\n<td>24</td>\n<td>64.56</td>\n<td>1</td>\n<td>0</td>\n<td>1.14286</td>\n</tr>\n<tr>\n<td>2</td>\n<td>1.0</td>\n<td>0</td>\n<td>0</td>\n<td>1.0</td>\n</tr>\n<tr>\n<td>2</td>\n<td>14.0</td>\n<td>0</td>\n<td>0</td>\n<td>1.14286</td>\n</tr>\n<tr>\n<td>...</td>\n</tr>\n<tr>\n<td>8</td>\n<td>0.0</td>\n<td>0</td>\n<td>1</td>\n<td>1.0</td>\n</tr>\n</table>\n\n\n<pre><code class=\"language-julia\">using TuringGLM</code></pre>\n\n\n\n<div class=\"markdown\"><p>Using <code>y</code> as dependent variable and <code>roach1</code>, <code>treatment</code>, and <code>senior</code> as independent variables:</p>\n</div>\n\n<pre><code class=\"language-julia\">fm = @formula(y ~ roach1 + treatment + senior)</code></pre>\n<pre><code class=\"code-output\">FormulaTerm\nResponse:\n  y(unknown)\nPredictors:\n  roach1(unknown)\n  treatment(unknown)\n  senior(unknown)</code></pre>\n\n\n<div class=\"markdown\"><p>We instantiate our model with <code>turing_model</code> passing a third argument <code>Pois&#40;&#41;</code> to indicate that the model is a Poisson Regression</p>\n</div>\n\n<pre><code class=\"language-julia\">model = turing_model(fm, roaches, Pois());</code></pre>\n\n\n\n<div class=\"markdown\"><p>Sample the model using the <code>NUTS</code> sampler and 2,000 samples:</p>\n</div>\n\n<pre><code class=\"language-julia\">chn = sample(model, NUTS(), 2_000);</code></pre>\n\n\n<pre><code class=\"language-julia\">describe(chn)[1]</code></pre>\n<table>\n<tr>\n<th>parameters</th>\n<th>mean</th>\n<th>std</th>\n<th>naive_se</th>\n<th>mcse</th>\n<th>ess</th>\n<th>rhat</th>\n<th>ess_per_sec</th>\n</tr>\n<tr>\n<td>:α</td>\n<td>3.13435</td>\n<td>0.0208904</td>\n<td>0.000467123</td>\n<td>0.000732099</td>\n<td>896.558</td>\n<td>0.9996</td>\n<td>49.229</td>\n</tr>\n<tr>\n<td>Symbol(\"β[1]\")</td>\n<td>0.00644871</td>\n<td>8.59555e-5</td>\n<td>1.92202e-6</td>\n<td>2.05367e-6</td>\n<td>1531.03</td>\n<td>0.9995</td>\n<td>84.0672</td>\n</tr>\n<tr>\n<td>Symbol(\"β[2]\")</td>\n<td>-0.511367</td>\n<td>0.0244623</td>\n<td>0.000546994</td>\n<td>0.000838555</td>\n<td>729.663</td>\n<td>1.00139</td>\n<td>40.065</td>\n</tr>\n<tr>\n<td>Symbol(\"β[3]\")</td>\n<td>-0.375404</td>\n<td>0.0328884</td>\n<td>0.000735407</td>\n<td>0.00112212</td>\n<td>808.176</td>\n<td>1.0006</td>\n<td>44.376</td>\n</tr>\n</table>\n\n\n\n<div class=\"markdown\"><h2>References</h2>\n<p>Gelman, A., &amp; Hill, J. &#40;2007&#41;. Data analysis using regression and multilevel/hierarchical models. Cambridge university press.</p>\n</div>\n\n<!-- PlutoStaticHTML.End -->","category":"page"}]
}
